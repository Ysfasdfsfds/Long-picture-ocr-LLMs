#!/usr/bin/env python3
"""
OCRå‚æ•°å®éªŒè„šæœ¬
æµ‹è¯•ä¸åŒçš„Detectionå‚æ•°é…ç½®å¯¹OCRæ€§èƒ½çš„å½±å“
"""

import os
import shutil
import time
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

from src.main import LongImageOCR

class OCRExperimentRunner:
    """OCRå‚æ•°å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, base_config_path: str = "default_rapidocr.yaml", 
                 experiments_dir: str = "/home/kylin/æ¡Œé¢/Long-picture-ocr-LLMs-main_a/experiments_results"):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨
        
        Args:
            base_config_path: åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„
            experiments_dir: å®éªŒç»“æœä¿å­˜ç›®å½•
        """
        self.base_config_path = base_config_path
        self.experiments_dir = Path(experiments_dir)
        self.base_config = self._load_base_config()
        
        # å®éªŒå‚æ•°ç»„åˆ
        self.experiment_configs = [
            {"limit_type": "max", "limit_side_len": 1200},
            {"limit_type": "max", "limit_side_len": 1000},
            {"limit_type": "max", "limit_side_len": 800},
            {"limit_type": "min", "limit_side_len": 1200},
            {"limit_type": "min", "limit_side_len": 1000},
            {"limit_type": "min", "limit_side_len": 800},
            {"limit_type": "max", "limit_side_len": 600},
            {"limit_type": "min", "limit_side_len": 600},
            {"limit_type": "max", "limit_side_len": 400},
            {"limit_type": "min", "limit_side_len": 400},
            {"limit_type": "max", "limit_side_len": 200},
            {"limit_type": "min", "limit_side_len": 200},
        ]
        
        # å®éªŒç»“æœ
        self.experiment_results = {}
    
    def _load_base_config(self) -> Dict:
        """åŠ è½½åŸºç¡€é…ç½®æ–‡ä»¶"""
        with open(self.base_config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _create_experiment_config(self, limit_type: str, limit_side_len: int) -> str:
        """
        åˆ›å»ºå®éªŒé…ç½®æ–‡ä»¶
        
        Args:
            limit_type: limitç±»å‹ (max/min)
            limit_side_len: limitè¾¹é•¿
            
        Returns:
            é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # å¤åˆ¶åŸºç¡€é…ç½®
        config = self.base_config.copy()
        
        # ä¿®æ”¹Detå‚æ•°
        config['Det']['limit_type'] = limit_type
        config['Det']['limit_side_len'] = limit_side_len
        
        # åˆ›å»ºå®éªŒç›®å½•
        exp_name = f"{limit_type}_{limit_side_len}"
        exp_dir = self.experiments_dir / exp_name
        exp_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_path = exp_dir / "config.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        return str(config_path)
    
    def _run_single_experiment(self, limit_type: str, limit_side_len: int, 
                             image_path: str) -> Dict[str, Any]:
        """
        è¿è¡Œå•ä¸ªå®éªŒ
        
        Args:
            limit_type: limitç±»å‹
            limit_side_len: limitè¾¹é•¿
            image_path: å›¾åƒè·¯å¾„
            
        Returns:
            å®éªŒç»“æœ
        """
        exp_name = f"{limit_type}_{limit_side_len}"
        exp_dir = self.experiments_dir / exp_name
        
        logger.info(f"ğŸ” å¼€å§‹å®éªŒ: {exp_name}")
        logger.info(f"  å‚æ•°: limit_type={limit_type}, limit_side_len={limit_side_len}")
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        config_path = self._create_experiment_config(limit_type, limit_side_len)
        
        # åˆ‡æ¢åˆ°å®éªŒç›®å½•
        original_cwd = os.getcwd()
        os.chdir(exp_dir)
        
        try:
            # æ¸…ç†ä¹‹å‰çš„è¾“å‡º
            if os.path.exists("output_images"):
                shutil.rmtree("output_images")
            if os.path.exists("output_json"):
                shutil.rmtree("output_json")
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # åˆå§‹åŒ–OCRå¤„ç†å™¨
            processor = LongImageOCR(config_path=config_path)
            
            # å¤„ç†å›¾åƒ
            result = processor.process_long_image(image_path)
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            total_time = end_time - start_time
            
            # æ”¶é›†å®éªŒç»“æœ
            experiment_result = {
                "experiment_name": exp_name,
                "parameters": {
                    "limit_type": limit_type,
                    "limit_side_len": limit_side_len
                },
                "timestamp": datetime.now().isoformat(),
                "total_experiment_time": round(total_time, 3),
                "processing_results": result,
                "image_path": image_path,
                "config_path": config_path
            }
            
            # è¯»å–è¯¦ç»†æ—¶é—´ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            timing_file = Path("output_json") / "timing_analysis.json"
            if timing_file.exists():
                with open(timing_file, 'r', encoding='utf-8') as f:
                    experiment_result["detailed_timing"] = json.load(f)
            
            # è¯»å–åˆ‡ç‰‡è¯¦ç»†è®¡æ—¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            slice_timing_file = Path("output_json") / "slice_ocr_detailed_timing.json"
            if slice_timing_file.exists():
                with open(slice_timing_file, 'r', encoding='utf-8') as f:
                    experiment_result["slice_detailed_timing"] = json.load(f)
            
            # ä¿å­˜å®éªŒç»“æœ
            result_file = exp_dir / "experiment_result.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(experiment_result, f, indent=2)
            
            # ç”Ÿæˆç®€è¦æŠ¥å‘Š
            self._generate_experiment_report(experiment_result, exp_dir)
            
            logger.info(f"âœ… å®éªŒ {exp_name} å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’")
            
            return experiment_result
            
        except Exception as e:
            logger.error(f"âŒ å®éªŒ {exp_name} å¤±è´¥: {e}")
            return {
                "experiment_name": exp_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
        finally:
            # æ¢å¤åŸå·¥ä½œç›®å½•
            os.chdir(original_cwd)
    
    def _generate_experiment_report(self, result: Dict, exp_dir: Path):
        """ç”Ÿæˆå•ä¸ªå®éªŒçš„æŠ¥å‘Š"""
        report_lines = []
        report_lines.append(f"# å®éªŒæŠ¥å‘Š: {result['experiment_name']}")
        report_lines.append(f"æ—¶é—´: {result['timestamp']}")
        report_lines.append("")
        
        # å‚æ•°ä¿¡æ¯
        report_lines.append("## å®éªŒå‚æ•°")
        for key, value in result['parameters'].items():
            report_lines.append(f"- {key}: {value}")
        report_lines.append("")
        
        # å¤„ç†ç»“æœ
        if 'processing_results' in result:
            pr = result['processing_results']
            report_lines.append("## å¤„ç†ç»“æœ")
            report_lines.append(f"- æ€»OCRé¡¹: {pr.get('total_ocr_items', 'N/A')}")
            report_lines.append(f"- æ€»å¤´åƒæ•°: {pr.get('total_avatar_items', 'N/A')}")
            report_lines.append(f"- æ€»æ¶ˆæ¯æ•°: {pr.get('total_chat_messages', 'N/A')}")
            report_lines.append(f"- æ€»æ‰§è¡Œæ—¶é—´: {pr.get('total_execution_time', 'N/A')}ç§’")
            report_lines.append("")
        
        # è¯¦ç»†æ—¶é—´ç»Ÿè®¡
        if 'detailed_timing' in result:
            dt = result['detailed_timing']
            report_lines.append("## æ—¶é—´ç»Ÿè®¡")
            if 'step_summary' in dt:
                for step, data in dt['step_summary'].items():
                    if isinstance(data, dict) and 'time' in data:
                        report_lines.append(f"- {data.get('description', step)}: {data['time']}ç§’")
            report_lines.append("")
        
        # åˆ‡ç‰‡ç»Ÿè®¡
        if 'slice_detailed_timing' in result:
            sdt = result['slice_detailed_timing']
            if 'summary' in sdt:
                summary = sdt['summary']
                report_lines.append("## åˆ‡ç‰‡å¤„ç†ç»Ÿè®¡")
                report_lines.append(f"- æ€»åˆ‡ç‰‡æ•°: {summary.get('total_slices', 'N/A')}")
                
                if 'detailed_timing_summary' in summary:
                    dts = summary['detailed_timing_summary']
                    report_lines.append(f"- Detectionæ€»æ—¶é—´: {dts.get('total_detection_time', 'N/A')}ç§’")
                    report_lines.append(f"- Recognitionæ€»æ—¶é—´: {dts.get('total_recognition_time', 'N/A')}ç§’")
                    report_lines.append(f"- å¹³å‡æ¯ç‰‡æ—¶é—´: {dts.get('average_total_time', 'N/A')}ç§’")
                
                if 'processing_summary' in summary:
                    ps = summary['processing_summary']
                    report_lines.append(f"- æ€»æ£€æµ‹æ¡†æ•°: {ps.get('total_detected_boxes', 'N/A')}")
                    report_lines.append(f"- æ€»è¯†åˆ«æ–‡æœ¬æ•°: {ps.get('total_final_texts', 'N/A')}")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = exp_dir / "timing_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
    
    def run_all_experiments(self, image_path: str):
        """
        è¿è¡Œæ‰€æœ‰å®éªŒé…ç½®
        
        Args:
            image_path: è¦å¤„ç†çš„å›¾åƒè·¯å¾„
        """
        logger.info(f"ğŸš€ å¼€å§‹è¿è¡ŒOCRå‚æ•°å®éªŒ")
        logger.info(f"ğŸ“· å›¾åƒè·¯å¾„: {image_path}")
        logger.info(f"ğŸ“ ç»“æœä¿å­˜è·¯å¾„: {self.experiments_dir}")
        logger.info(f"ğŸ”¢ å®éªŒé…ç½®æ•°é‡: {len(self.experiment_configs)}")
        
        # åˆ›å»ºå®éªŒæ ¹ç›®å½•
        self.experiments_dir.mkdir(parents=True, exist_ok=True)
        
        # è¿è¡Œæ‰€æœ‰å®éªŒ
        for i, config in enumerate(self.experiment_configs, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ§ª å®éªŒ {i}/{len(self.experiment_configs)}")
            
            result = self._run_single_experiment(
                config['limit_type'], 
                config['limit_side_len'], 
                image_path
            )
            
            exp_name = f"{config['limit_type']}_{config['limit_side_len']}"
            self.experiment_results[exp_name] = result
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self._generate_summary_report()
        
        # ç”Ÿæˆè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š
        self._generate_detailed_comparison_report()
        
        # ç”ŸæˆRecognitionè¯¦ç»†åˆ†ææŠ¥å‘Š
        self._generate_recognition_detailed_report()
        
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼")
        logger.info(f"ğŸ“Š æ±‡æ€»æŠ¥å‘Šä¿å­˜åœ¨: {self.experiments_dir / 'summary_report.json'}")
        logger.info(f"ğŸ“Š è¯¦ç»†å¯¹æ¯”æŠ¥å‘Šä¿å­˜åœ¨: {self.experiments_dir / 'detailed_comparison_report.txt'}")
        logger.info(f"ğŸ“Š Recognitionè¯¦ç»†åˆ†æä¿å­˜åœ¨: {self.experiments_dir / 'recognition_detailed_report.txt'}")
    
    def _generate_summary_report(self):
        """ç”Ÿæˆæ±‡æ€»å¯¹æ¯”æŠ¥å‘Š"""
        summary = {
            "experiment_overview": {
                "total_experiments": len(self.experiment_results),
                "timestamp": datetime.now().isoformat(),
                "experiment_configs": self.experiment_configs
            },
            "results": self.experiment_results,
            "performance_comparison": self._analyze_performance()
        }
        
        # ä¿å­˜æ±‡æ€»JSON
        summary_file = self.experiments_dir / "summary_report.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2)
        
        # ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„å¯¹æ¯”æŠ¥å‘Š
        self._generate_text_comparison_report(summary)
    
    def _analyze_performance(self) -> Dict:
        """åˆ†ææ€§èƒ½å¯¹æ¯”"""
        analysis = {
            "fastest_experiment": None,
            "slowest_experiment": None,
            "detection_time_comparison": {},
            "recognition_time_comparison": {},
            "accuracy_comparison": {}
        }
        
        valid_results = {k: v for k, v in self.experiment_results.items() 
                        if 'error' not in v and 'processing_results' in v}
        
        if not valid_results:
            return analysis
        
        # æ‰¾å‡ºæœ€å¿«å’Œæœ€æ…¢çš„å®éªŒ
        times = {k: v.get('total_experiment_time', float('inf')) 
                for k, v in valid_results.items()}
        
        if times:
            fastest = min(times, key=times.get)
            slowest = max(times, key=times.get)
            
            analysis["fastest_experiment"] = {
                "name": fastest,
                "time": times[fastest]
            }
            analysis["slowest_experiment"] = {
                "name": slowest,
                "time": times[slowest]
            }
        
        # æ¯”è¾ƒDetectionå’ŒRecognitionæ—¶é—´
        for exp_name, result in valid_results.items():
            if 'slice_detailed_timing' in result and 'summary' in result['slice_detailed_timing']:
                summary = result['slice_detailed_timing']['summary']
                if 'detailed_timing_summary' in summary:
                    dts = summary['detailed_timing_summary']
                    analysis["detection_time_comparison"][exp_name] = dts.get('total_detection_time', 0)
                    analysis["recognition_time_comparison"][exp_name] = dts.get('total_recognition_time', 0)
                
                if 'processing_summary' in summary:
                    ps = summary['processing_summary']
                    analysis["accuracy_comparison"][exp_name] = {
                        "detected_boxes": ps.get('total_detected_boxes', 0),
                        "final_texts": ps.get('total_final_texts', 0)
                    }
        
        return analysis
    
    def _generate_text_comparison_report(self, summary: Dict):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„å¯¹æ¯”æŠ¥å‘Š"""
        report_lines = []
        report_lines.append("# OCRå‚æ•°å®éªŒå¯¹æ¯”æŠ¥å‘Š")
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {summary['experiment_overview']['timestamp']}")
        report_lines.append(f"å®éªŒæ€»æ•°: {summary['experiment_overview']['total_experiments']}")
        report_lines.append("")
        
        # æ€§èƒ½å¯¹æ¯”
        perf = summary['performance_comparison']
        if perf.get('fastest_experiment') and perf.get('slowest_experiment'):
            report_lines.append("## æ€§èƒ½å¯¹æ¯”")
            report_lines.append(f"æœ€å¿«å®éªŒ: {perf['fastest_experiment']['name']} ({perf['fastest_experiment']['time']:.2f}ç§’)")
            report_lines.append(f"æœ€æ…¢å®éªŒ: {perf['slowest_experiment']['name']} ({perf['slowest_experiment']['time']:.2f}ç§’)")
            report_lines.append("")
        
        # Detectionæ—¶é—´å¯¹æ¯”
        if perf.get('detection_time_comparison'):
            report_lines.append("## Detectionæ—¶é—´å¯¹æ¯”")
            det_times = perf['detection_time_comparison']
            sorted_det = sorted(det_times.items(), key=lambda x: x[1])
            for exp_name, time_val in sorted_det:
                report_lines.append(f"- {exp_name}: {time_val:.3f}ç§’")
            report_lines.append("")
        
        # Recognitionæ—¶é—´å¯¹æ¯”
        if perf.get('recognition_time_comparison'):
            report_lines.append("## Recognitionæ—¶é—´å¯¹æ¯”")
            rec_times = perf['recognition_time_comparison']
            sorted_rec = sorted(rec_times.items(), key=lambda x: x[1])
            for exp_name, time_val in sorted_rec:
                report_lines.append(f"- {exp_name}: {time_val:.3f}ç§’")
            report_lines.append("")
        
        # ç²¾åº¦å¯¹æ¯”
        if perf.get('accuracy_comparison'):
            report_lines.append("## è¯†åˆ«ç²¾åº¦å¯¹æ¯”")
            acc_comp = perf['accuracy_comparison']
            for exp_name, acc_data in acc_comp.items():
                detection_count = acc_data.get('detected_boxes', 0)
                final_count = acc_data.get('final_texts', 0)
                efficiency = (final_count / detection_count * 100) if detection_count > 0 else 0
                report_lines.append(f"- {exp_name}: æ£€æµ‹æ¡†{detection_count} â†’ æœ€ç»ˆæ–‡æœ¬{final_count} (æ•ˆç‡{efficiency:.1f}%)")
            report_lines.append("")
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        text_report_file = self.experiments_dir / "comparison_report.txt"
        with open(text_report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
    
    def _generate_detailed_comparison_report(self):
        """ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Šï¼ŒåŒ…å«å®Œæ•´æ—¶é—´åˆ†æ"""
        from datetime import datetime
        
        report_lines = []
        report_lines.append("# OCRå‚æ•°å®éªŒè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š")
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"å®éªŒæ€»æ•°: {len(self.experiment_results)}ç§é…ç½®")
        report_lines.append("")
        
        # æ”¶é›†æ‰€æœ‰å®éªŒæ•°æ®
        experiment_data = {}
        for exp_name, result in self.experiment_results.items():
            if 'error' not in result and 'slice_detailed_timing' in result:
                sdt = result['slice_detailed_timing']
                if 'summary' in sdt and 'detailed_timing_summary' in sdt['summary']:
                    dts = sdt['summary']['detailed_timing_summary']
                    experiment_data[exp_name] = {
                        'detection_time': dts.get('total_detection_time', 0),
                        'recognition_time': dts.get('total_recognition_time', 0),
                        'total_experiment_time': result.get('total_experiment_time', 0),
                        'processing_results': result.get('processing_results', {})
                    }
        
        if not experiment_data:
            report_lines.append("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å®éªŒæ•°æ®")
            # ä¿å­˜æŠ¥å‘Š
            detailed_report_file = self.experiments_dir / "detailed_comparison_report.txt"
            with open(detailed_report_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_lines))
            return
        
        # ç”Ÿæˆè¡¨æ ¼
        report_lines.append("## ğŸ“Š å®Œæ•´æ—¶é—´å¯¹æ¯”è¡¨æ ¼")
        report_lines.append("")
        report_lines.append("| é…ç½® | Detectionæ—¶é—´(s) | Recognitionæ—¶é—´(s) | OCRæ€»æ—¶é—´(s) | å®éªŒæ€»æ—¶é—´(s) | å…¶ä»–æ—¶é—´(s) |")
        report_lines.append("|------|-----------------|------------------|-------------|-------------|------------|")
        
        for exp_name in sorted(experiment_data.keys()):
            data = experiment_data[exp_name]
            det_time = data['detection_time']
            rec_time = data['recognition_time']
            ocr_total = det_time + rec_time
            exp_total = data['total_experiment_time']
            other_time = exp_total - ocr_total
            
            report_lines.append(f"| {exp_name:<8} | {det_time:>6.3f} | {rec_time:>8.3f} | {ocr_total:>7.3f} | {exp_total:>7.2f} | {other_time:>6.2f} |")
        
        report_lines.append("")
        
        # æ€§èƒ½æ’å
        report_lines.append("## ğŸ† æ€§èƒ½æ’å")
        report_lines.append("")
        
        # æŒ‰æ€»æ—¶é—´æ’åº
        sorted_by_total = sorted(experiment_data.items(), key=lambda x: x[1]['total_experiment_time'])
        report_lines.append("### æŒ‰æ€»æ—¶é—´æ’åº (å¿«â†’æ…¢)")
        for i, (exp_name, data) in enumerate(sorted_by_total, 1):
            total_time = data['total_experiment_time']
            if i == 1:
                report_lines.append(f"{i}. **{exp_name}**: {total_time:.2f}ç§’ â­ æœ€ä½³")
            elif i == len(sorted_by_total):
                report_lines.append(f"{i}. {exp_name}: {total_time:.2f}ç§’ ğŸŒ æœ€æ…¢")
            else:
                report_lines.append(f"{i}. {exp_name}: {total_time:.2f}ç§’")
        report_lines.append("")
        
        # æŒ‰Detectionæ—¶é—´æ’åº
        sorted_by_detection = sorted(experiment_data.items(), key=lambda x: x[1]['detection_time'])
        report_lines.append("### æŒ‰Detectionæ—¶é—´æ’åº (å¿«â†’æ…¢)")
        for i, (exp_name, data) in enumerate(sorted_by_detection, 1):
            det_time = data['detection_time']
            if i == 1:
                report_lines.append(f"{i}. **{exp_name}**: {det_time:.3f}ç§’ âš¡ æœ€å¿«Detection")
            else:
                report_lines.append(f"{i}. {exp_name}: {det_time:.3f}ç§’")
        report_lines.append("")
        
        # æŒ‰Recognitionæ—¶é—´æ’åº
        sorted_by_recognition = sorted(experiment_data.items(), key=lambda x: x[1]['recognition_time'])
        report_lines.append("### æŒ‰Recognitionæ—¶é—´æ’åº (å¿«â†’æ…¢)")
        for i, (exp_name, data) in enumerate(sorted_by_recognition, 1):
            rec_time = data['recognition_time']
            if i == 1:
                report_lines.append(f"{i}. **{exp_name}**: {rec_time:.3f}ç§’ âš¡ æœ€å¿«Recognition")
            else:
                report_lines.append(f"{i}. {exp_name}: {rec_time:.3f}ç§’")
        report_lines.append("")
        
        # å…³é”®å‘ç°
        report_lines.append("## ğŸ“ˆ å…³é”®å‘ç°")
        report_lines.append("")
        
        # åˆ†ælimit_typeå½±å“
        min_configs = {k: v for k, v in experiment_data.items() if k.startswith('min_')}
        max_configs = {k: v for k, v in experiment_data.items() if k.startswith('max_')}
        
        if min_configs and max_configs:
            avg_min_det = sum(v['detection_time'] for v in min_configs.values()) / len(min_configs)
            avg_max_det = sum(v['detection_time'] for v in max_configs.values()) / len(max_configs)
            avg_min_rec = sum(v['recognition_time'] for v in min_configs.values()) / len(min_configs)
            avg_max_rec = sum(v['recognition_time'] for v in max_configs.values()) / len(max_configs)
            
            report_lines.append("### 1. limit_typeå½±å“åˆ†æ")
            report_lines.append(f"- **minç³»åˆ—** ({avg_min_det:.1f}s Detection, {avg_min_rec:.1f}s Recognition)")
            report_lines.append(f"- **maxç³»åˆ—** ({avg_max_det:.1f}s Detection, {avg_max_rec:.1f}s Recognition)")
            det_improvement = ((avg_max_det - avg_min_det) / avg_max_det) * 100
            rec_improvement = ((avg_max_rec - avg_min_rec) / avg_max_rec) * 100
            report_lines.append(f"- **ç»“è®º**: `limit_type=min` æ¯” `limit_type=max` åœ¨Detectionå¿«{det_improvement:.0f}%, Recognitionå¿«{rec_improvement:.0f}%")
            report_lines.append("")
        
        # æœ€ä¼˜é…ç½®æ¨è
        best_config = sorted_by_total[0]
        best_name = best_config[0]
        best_data = best_config[1]
        
        report_lines.append("### 2. æœ€ä¼˜é…ç½®æ¨è")
        report_lines.append(f"**ğŸ¥‡ ç»¼åˆæœ€ä½³**: `{best_name}`")
        report_lines.append(f"- æ€»æ—¶é—´æœ€çŸ­: {best_data['total_experiment_time']:.2f}ç§’")
        report_lines.append(f"- Detectionæ—¶é—´: {best_data['detection_time']:.3f}ç§’")
        report_lines.append(f"- Recognitionæ—¶é—´: {best_data['recognition_time']:.3f}ç§’")
        
        # ä»processing_resultsä¸­è·å–è¯†åˆ«æ•ˆç‡
        if 'timing' in best_data['processing_results']:
            pr = best_data['processing_results']
            total_ocr = pr.get('total_ocr_items', 0)
            if 'step3_summary' in pr.get('timing', {}):
                step3 = pr['timing']['step3_summary']
                if 'total_ocr_time' in step3:
                    efficiency = total_ocr / step3['total_ocr_time'] if step3['total_ocr_time'] > 0 else 0
                    report_lines.append(f"- è¯†åˆ«æ•ˆç‡: {efficiency:.1f} é¡¹/ç§’")
        report_lines.append("")
        
        # æ€§èƒ½å·®å¼‚åˆ†æ
        worst_config = sorted_by_total[-1]
        time_diff = worst_config[1]['total_experiment_time'] - best_data['total_experiment_time']
        improvement_pct = (time_diff / worst_config[1]['total_experiment_time']) * 100
        
        report_lines.append("## ğŸ” æ€§èƒ½å·®å¼‚åˆ†æ")
        fastest_det = sorted_by_detection[0][1]['detection_time']
        slowest_det = sorted_by_detection[-1][1]['detection_time']
        fastest_rec = sorted_by_recognition[0][1]['recognition_time']
        slowest_rec = sorted_by_recognition[-1][1]['recognition_time']
        
        report_lines.append(f"- æœ€å¿« vs æœ€æ…¢: **{time_diff:.2f}ç§’å·®å¼‚** ({improvement_pct:.0f}%æ€§èƒ½å·®å¼‚)")
        report_lines.append(f"- Detectionå·®å¼‚: **{slowest_det - fastest_det:.1f}ç§’** ({((slowest_det - fastest_det) / slowest_det * 100):.0f}%å·®å¼‚)")
        report_lines.append(f"- Recognitionå·®å¼‚: **{slowest_rec - fastest_rec:.1f}ç§’** ({((slowest_rec - fastest_rec) / slowest_rec * 100):.0f}%å·®å¼‚)")
        report_lines.append("")
        
        # ä½¿ç”¨å»ºè®®
        report_lines.append("## ğŸ’¡ ä½¿ç”¨å»ºè®®")
        report_lines.append(f"1. **ç”Ÿäº§ç¯å¢ƒæ¨è**: `{best_name}` - æœ€ä½³å¹³è¡¡ç‚¹")
        if sorted_by_detection[0][0] != best_name:
            report_lines.append(f"2. **è¿½æ±‚æè‡´Detectioné€Ÿåº¦**: `{sorted_by_detection[0][0]}`")
        report_lines.append(f"3. **é¿å…ä½¿ç”¨**: `{worst_config[0]}` - æ€§èƒ½æœ€å·®")
        
        limit_type_recommendation = "min" if best_name.startswith('min_') else "max"
        report_lines.append(f"4. **limit_typeé€‰æ‹©**: æ¨èä½¿ç”¨ `{limit_type_recommendation}`ï¼Œæ€§èƒ½æå‡æ˜¾è‘—")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        detailed_report_file = self.experiments_dir / "detailed_comparison_report.txt"
        with open(detailed_report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        logger.info(f"è¯¦ç»†å¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ: {detailed_report_file}")
    
    def _generate_recognition_detailed_report(self):
        """ç”ŸæˆRecognitioné˜¶æ®µè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š"""
        from datetime import datetime
        
        report_lines = []
        report_lines.append("# Recognitionæ€§èƒ½è¯¦ç»†åˆ†ææŠ¥å‘Š")
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"å®éªŒæ€»æ•°: {len(self.experiment_results)}ç§é…ç½®")
        report_lines.append("")
        
        # æ”¶é›†æ‰€æœ‰å®éªŒçš„Recognitionè¯¦ç»†æ•°æ®
        recognition_data = {}
        slice_count = 0
        
        for exp_name, result in self.experiment_results.items():
            if 'error' not in result and 'slice_detailed_timing' in result:
                sdt = result['slice_detailed_timing']
                if 'slice_details' in sdt:
                    recognition_data[exp_name] = {}
                    slice_details = sdt['slice_details']
                    slice_count = max(slice_count, len(slice_details))
                    
                    for slice_idx_str, slice_data in slice_details.items():
                        slice_idx = int(slice_idx_str)
                        recognition_data[exp_name][slice_idx] = {
                            'input_boxes': slice_data.get('detected_boxes_count', 0),
                            'recognition_time': slice_data.get('recognition_time', 0),
                            'recognition_detailed': slice_data.get('recognition_detailed', {}),
                            'start_y': slice_data.get('start_y', slice_idx * 1000),
                            'end_y': slice_data.get('end_y', (slice_idx + 1) * 1000)
                        }
        
        if not recognition_data:
            report_lines.append("âŒ æ²¡æœ‰æ‰¾åˆ°Recognitionè¯¦ç»†æ•°æ®")
            recognition_report_file = self.experiments_dir / "recognition_detailed_report.txt"
            with open(recognition_report_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_lines))
            return
        
        # ç”Ÿæˆæ¯ä¸ªåˆ‡ç‰‡çš„è¯¦ç»†å¯¹æ¯”
        report_lines.append("## ğŸ“Š åˆ‡ç‰‡çº§Recognitionæ€§èƒ½å¯¹æ¯”")
        report_lines.append("")
        
        configs = sorted(recognition_data.keys())
        
        # ä¸ºå‰10ä¸ªåˆ‡ç‰‡ç”Ÿæˆè¯¦ç»†å¯¹æ¯”ï¼ˆé¿å…æŠ¥å‘Šè¿‡é•¿ï¼‰
        for slice_idx in range(min(10, slice_count)):
            slice_data_exists = any(slice_idx in recognition_data[config] for config in configs)
            if not slice_data_exists:
                continue
                
            # è·å–åˆ‡ç‰‡çš„yåæ ‡èŒƒå›´
            y_start = None
            y_end = None
            for config in configs:
                if slice_idx in recognition_data[config]:
                    y_start = recognition_data[config][slice_idx]['start_y']
                    y_end = recognition_data[config][slice_idx]['end_y']
                    break
            
            report_lines.append(f"### åˆ‡ç‰‡{slice_idx} (y: {y_start}-{y_end}) Recognitionè¯¦ç»†å¯¹æ¯”")
            report_lines.append("")
            
            # åŸºç¡€ä¿¡æ¯è¡¨æ ¼
            report_lines.append("#### åŸºç¡€ä¿¡æ¯")
            report_lines.append("| é…ç½® | æ–‡æœ¬æ¡†æ•° | Recognitionæ€»æ—¶é—´(s) |")
            report_lines.append("|------|---------|---------------------|")
            
            for config in configs:
                if slice_idx in recognition_data[config]:
                    data = recognition_data[config][slice_idx]
                    boxes_count = data['input_boxes']
                    total_time = data['recognition_time']
                    report_lines.append(f"| {config:<8} | {boxes_count:>8} | {total_time:>11.3f} |")
            
            report_lines.append("")
            
            # è¯¦ç»†æ—¶é—´åˆ†è§£è¡¨æ ¼
            report_lines.append("#### Recognitioné˜¶æ®µåˆ†è§£")
            report_lines.append("| é…ç½® | é¢„å¤„ç† | | | Forward | åå¤„ç† | |")
            report_lines.append("|------|--------|-------|-------|---------|--------|-------|")
            report_lines.append("| | resize | batch | å°è®¡ | æ¨ç† | CTCè§£ç  | å°è®¡ |")
            
            for config in configs:
                if slice_idx in recognition_data[config]:
                    data = recognition_data[config][slice_idx]
                    detailed = data.get('recognition_detailed', {})
                    
                    # æå–å„é˜¶æ®µæ—¶é—´
                    prep = detailed.get('preprocessing', {})
                    resize_time = prep.get('resize_norm_time', 0)
                    batch_time = prep.get('batch_prepare_time', 0)
                    prep_total = prep.get('total', resize_time + batch_time)
                    
                    forward = detailed.get('forward', {})
                    inference_time = forward.get('inference_time', 0)
                    
                    post = detailed.get('postprocessing', {})
                    ctc_time = post.get('ctc_decode_time', 0)
                    post_total = post.get('total', ctc_time)
                    
                    report_lines.append(f"| {config:<8} | {resize_time:>6.3f} | {batch_time:>5.3f} | {prep_total:>5.3f} | {inference_time:>7.3f} | {ctc_time:>7.3f} | {post_total:>5.3f} |")
            
            report_lines.append("")
            
            # æ€§èƒ½å·®å¼‚åˆ†æ
            if len(configs) >= 2:
                report_lines.append("#### å…³é”®å‘ç°")
                
                # æ‰¾å‡ºæœ€å¿«å’Œæœ€æ…¢çš„é…ç½®
                valid_configs = [(config, recognition_data[config][slice_idx]['recognition_time']) 
                               for config in configs if slice_idx in recognition_data[config]]
                
                if valid_configs:
                    fastest = min(valid_configs, key=lambda x: x[1])
                    slowest = max(valid_configs, key=lambda x: x[1])
                    
                    time_diff = slowest[1] - fastest[1]
                    improvement_pct = (time_diff / slowest[1]) * 100 if slowest[1] > 0 else 0
                    
                    report_lines.append(f"- **æœ€å¿«é…ç½®**: {fastest[0]} ({fastest[1]:.3f}s)")
                    report_lines.append(f"- **æœ€æ…¢é…ç½®**: {slowest[0]} ({slowest[1]:.3f}s)")
                    report_lines.append(f"- **æ€§èƒ½å·®å¼‚**: {time_diff:.3f}s ({improvement_pct:.0f}%å·®å¼‚)")
                    
                    # åˆ†æä¸»è¦ç“¶é¢ˆ
                    fastest_detailed = recognition_data[fastest[0]][slice_idx].get('recognition_detailed', {})
                    slowest_detailed = recognition_data[slowest[0]][slice_idx].get('recognition_detailed', {})
                    
                    fastest_inference = fastest_detailed.get('forward', {}).get('inference_time', 0)
                    slowest_inference = slowest_detailed.get('forward', {}).get('inference_time', 0)
                    
                    if fastest_inference > 0 and slowest_inference > 0:
                        inference_diff_pct = ((slowest_inference - fastest_inference) / fastest_inference) * 100
                        report_lines.append(f"- **æ¨ç†é˜¶æ®µå·®å¼‚**: {inference_diff_pct:.0f}% (ä¸»è¦æ€§èƒ½ç“¶é¢ˆ)")
            
            report_lines.append("")
            report_lines.append("---")
            report_lines.append("")
        
        # æ•´ä½“ç»Ÿè®¡åˆ†æ
        report_lines.append("## ğŸ“ˆ æ•´ä½“Recognitionæ€§èƒ½ç»Ÿè®¡")
        report_lines.append("")
        
        # è®¡ç®—å„é…ç½®çš„å¹³å‡æ€§èƒ½
        avg_performance = {}
        for config in configs:
            total_recognition_time = 0
            total_boxes = 0
            total_slices = 0
            
            for slice_idx, data in recognition_data[config].items():
                total_recognition_time += data['recognition_time']
                total_boxes += data['input_boxes']
                total_slices += 1
            
            if total_slices > 0:
                avg_performance[config] = {
                    'avg_recognition_time': total_recognition_time / total_slices,
                    'avg_boxes_per_slice': total_boxes / total_slices,
                    'avg_time_per_box': total_recognition_time / total_boxes if total_boxes > 0 else 0
                }
        
        report_lines.append("### å¹³å‡æ€§èƒ½å¯¹æ¯”")
        report_lines.append("| é…ç½® | å¹³å‡Recognitionæ—¶é—´(s) | å¹³å‡æ–‡æœ¬æ¡†æ•° | å¹³å‡æ¯æ¡†æ—¶é—´(ms) |")
        report_lines.append("|------|----------------------|-------------|----------------|")
        
        for config in sorted(avg_performance.keys()):
            perf = avg_performance[config]
            avg_time = perf['avg_recognition_time']
            avg_boxes = perf['avg_boxes_per_slice']
            time_per_box = perf['avg_time_per_box'] * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            report_lines.append(f"| {config:<8} | {avg_time:>14.3f} | {avg_boxes:>11.1f} | {time_per_box:>14.1f} |")
        
        report_lines.append("")
        
        # æ€§èƒ½æ’å
        report_lines.append("### ğŸ† Recognitionæ€§èƒ½æ’å")
        sorted_by_avg_time = sorted(avg_performance.items(), key=lambda x: x[1]['avg_recognition_time'])
        
        for i, (config, perf) in enumerate(sorted_by_avg_time, 1):
            avg_time = perf['avg_recognition_time']
            if i == 1:
                report_lines.append(f"{i}. **{config}**: {avg_time:.3f}s â­ æœ€å¿«Recognition")
            elif i == len(sorted_by_avg_time):
                report_lines.append(f"{i}. {config}: {avg_time:.3f}s ğŸŒ æœ€æ…¢Recognition")
            else:
                report_lines.append(f"{i}. {config}: {avg_time:.3f}s")
        
        report_lines.append("")
        
        # å…³é”®æ´å¯Ÿ
        if len(sorted_by_avg_time) >= 2:
            fastest_config = sorted_by_avg_time[0]
            slowest_config = sorted_by_avg_time[-1]
            
            performance_gap = ((slowest_config[1]['avg_recognition_time'] - 
                              fastest_config[1]['avg_recognition_time']) / 
                             slowest_config[1]['avg_recognition_time']) * 100
            
            report_lines.append("## ğŸ’¡ å…³é”®æ´å¯Ÿ")
            report_lines.append(f"1. **æœ€ä¼˜Recognitioné…ç½®**: `{fastest_config[0]}` - å¹³å‡æ¯åˆ‡ç‰‡ {fastest_config[1]['avg_recognition_time']:.3f}s")
            report_lines.append(f"2. **æ€§èƒ½å·®è·**: æœ€å¿«ä¸æœ€æ…¢é…ç½®ç›¸å·® {performance_gap:.0f}%")
            report_lines.append(f"3. **å•æ¡†å¹³å‡å¤„ç†æ—¶é—´**: {fastest_config[1]['avg_time_per_box']*1000:.1f}ms (æœ€å¿«) vs {slowest_config[1]['avg_time_per_box']*1000:.1f}ms (æœ€æ…¢)")
            
            # åˆ¤æ–­æ˜¯å¦ä¸ä¹‹å‰çš„æ•´ä½“å®éªŒç»“æœä¸€è‡´
            report_lines.append(f"4. **ä¸æ•´ä½“å®éªŒç»“æœå¯¹æ¯”**: Recognitionæœ€å¿«é…ç½®ä¸º `{fastest_config[0]}`")
        
        # ä¿å­˜Recognitionè¯¦ç»†æŠ¥å‘Š
        recognition_report_file = self.experiments_dir / "recognition_detailed_report.txt"
        with open(recognition_report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        logger.info(f"Recognitionè¯¦ç»†åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {recognition_report_file}")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    image_path = "/home/kylin/æ¡Œé¢/Long-picture-ocr-LLMs-main_a/images/image copy 18.png"  # ä½¿ç”¨ç”¨æˆ·æä¾›çš„å›¾ç‰‡
    
    # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(image_path):
        print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        print("è¯·ç¡®ä¿å›¾åƒæ–‡ä»¶è·¯å¾„æ­£ç¡®")
        return
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = OCRExperimentRunner()
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    try:
        runner.run_all_experiments(image_path)
        print("\nğŸ‰ å®éªŒå…¨éƒ¨å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {runner.experiments_dir}")
        print("ğŸ“Š æŸ¥çœ‹æ±‡æ€»æŠ¥å‘Š: summary_report.json")
        print("ğŸ“ˆ æŸ¥çœ‹å¯¹æ¯”æŠ¥å‘Š: comparison_report.txt")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®éªŒè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()