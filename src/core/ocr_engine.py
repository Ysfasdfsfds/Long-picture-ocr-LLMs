"""
OCRå¼•æ“å°è£…æ¨¡å—
æä¾›ç»Ÿä¸€çš„OCRæ¥å£ï¼Œæ”¯æŒåˆ†ç¦»detectionå’Œrecognitionçš„æ—¶é—´ç»Ÿè®¡
"""

import cv2
import numpy as np
import time
import json
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
from rapidocr import RapidOCR
from rapidocr.main import RapidOCR as RapidOCREngine
from rapidocr.ch_ppocr_det import TextDetector, TextDetOutput
from rapidocr.ch_ppocr_rec import TextRecognizer, TextRecOutput, TextRecInput
from rapidocr.ch_ppocr_cls import TextClassifier, TextClsOutput
from rapidocr.utils.parse_parameters import ParseParams
from rapidocr.utils import LoadImage, resize_image_within_bounds, get_rotate_crop_image
import copy
import logging
from PIL import Image, ImageDraw, ImageFont
import io

from ..models.ocr_result import OCRItem, SliceOCRResult
from ..models.slice_info import SliceInfo
from ..utils.config import Config
from .recognition_profiler import RecognitionProfiler, RecognitionTiming
from .text_recognizer_wrapper import TextRecognizerWrapper
from .text_detector_wrapper import TextDetectorWrapper

logger = logging.getLogger(__name__)


class OCREngine:
    """OCRå¼•æ“å°è£…ç±»ï¼Œæ”¯æŒåˆ†ç¦»detectionå’Œrecognitionçš„è¯¦ç»†è®¡æ—¶"""
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–OCRå¼•æ“
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.engine = RapidOCR(config_path=config.ocr.config_path)
        self.text_score_threshold = config.ocr.text_score_threshold
        
        # åˆå§‹åŒ–è¯¦ç»†è®¡æ—¶çš„RapidOCRç»„ä»¶
        self._init_detailed_ocr_components(config.ocr.config_path)
        
        # å­˜å‚¨æ¯ä¸ªåˆ‡ç‰‡çš„è¯¦ç»†è®¡æ—¶æ•°æ®
        self.slice_timing_records = {}
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.timing_output_dir = Path("output_json")
        self.timing_output_dir.mkdir(exist_ok=True)
    
    def _init_detailed_ocr_components(self, config_path: str):
        """
        åˆå§‹åŒ–ç”¨äºè¯¦ç»†è®¡æ—¶çš„OCRç»„ä»¶
        """
        # åŠ è½½é…ç½®
        root_dir = Path(__file__).resolve().parent / ".." / ".."
        default_cfg_path = Path("/home/kylin/miniconda3/envs/long_ocr_LLm/lib/python3.12/site-packages/rapidocr/config.yaml")
        
        if config_path and Path(config_path).exists():
            cfg = ParseParams.load(config_path)
        else:
            cfg = ParseParams.load(default_cfg_path)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        cfg.Det.engine_cfg = cfg.EngineConfig[cfg.Det.engine_type.value]
        self.text_det = TextDetector(cfg.Det)
        
        cfg.Cls.engine_cfg = cfg.EngineConfig[cfg.Cls.engine_type.value]
        self.text_cls = TextClassifier(cfg.Cls)
        
        cfg.Rec.engine_cfg = cfg.EngineConfig[cfg.Rec.engine_type.value]
        cfg.Rec.font_path = cfg.Global.font_path
        self.text_rec = TextRecognizer(cfg.Rec)
        
        self.load_img = LoadImage()
        self.max_side_len = cfg.Global.max_side_len
        self.min_side_len = cfg.Global.min_side_len
        self.width_height_ratio = cfg.Global.width_height_ratio
        self.min_height = cfg.Global.min_height
        
        self.cfg = cfg
        
        # åˆå§‹åŒ–æ€§èƒ½åˆ†æå™¨
        self.recognition_profiler = RecognitionProfiler()
        
        # åŒ…è£…TextDetectorä»¥æ”¯æŒè¯¦ç»†æ€§èƒ½åˆ†æ
        if hasattr(self, 'text_det'):
            self.text_det = TextDetectorWrapper(self.text_det, profiling_enabled=True)
        
        # åŒ…è£…TextRecognizerä»¥æ”¯æŒè¯¦ç»†æ€§èƒ½åˆ†æ
        if hasattr(self, 'text_rec'):
            self.text_rec = TextRecognizerWrapper(self.text_rec, profiling_enabled=True)
        
        logger.info("è¯¦ç»†è®¡æ—¶OCRç»„ä»¶åˆå§‹åŒ–å®Œæˆï¼ˆåŒ…å«Recognitionæ€§èƒ½åˆ†æï¼‰")
    
    def process_slice(self, slice_info: SliceInfo, 
                     save_visualization: bool = True) -> SliceOCRResult:
        """
        å¤„ç†å•ä¸ªåˆ‡ç‰‡çš„OCRè¯†åˆ«ï¼ˆä½¿ç”¨åŸæœ‰å¼•æ“ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰
        
        Args:
            slice_info: åˆ‡ç‰‡ä¿¡æ¯
            save_visualization: æ˜¯å¦ä¿å­˜å¯è§†åŒ–ç»“æœ
            
        Returns:
            åˆ‡ç‰‡OCRç»“æœ
        """
        logger.info(f"å¤„ç†åˆ‡ç‰‡ {slice_info.slice_index}...")
        
        # åˆ›å»ºç»“æœå¯¹è±¡
        result = SliceOCRResult(
            slice_index=slice_info.slice_index,
            start_y=slice_info.start_y,
            end_y=slice_info.end_y
        )
        
        # è¿›è¡ŒOCRè¯†åˆ«
        slice_img_rgb = cv2.cvtColor(slice_info.image, cv2.COLOR_BGR2RGB)
        ocr_result = self.engine(slice_img_rgb)
        
        # # ä¿å­˜å¯è§†åŒ–ç»“æœ
        # if save_visualization:
        #     vis_path = f"{self.config.output.output_images_dir}/slice_ocr_result_{slice_info.slice_index}.jpg"
        #     ocr_result.vis(vis_path)
        
        # å¤„ç†OCRç»“æœ
        if ocr_result.boxes is not None and ocr_result.txts is not None:
            self._process_ocr_items(ocr_result, slice_info, result)
        else:
            logger.warning(f"åˆ‡ç‰‡ {slice_info.slice_index} æœªæ£€æµ‹åˆ°æ–‡æœ¬")
        
        # æ’åºç»“æœ
        result.sort_by_y()
        
        # logger.info(f"åˆ‡ç‰‡ {slice_info.slice_index} å¤„ç†å®Œæˆï¼Œ"
        #            f"æ£€æµ‹åˆ° {len(result.ocr_items)} ä¸ªæ–‡æœ¬")
        
        return result
    
    def process_slice_with_detailed_timing(self, slice_info: SliceInfo, 
                                         save_visualization: bool = True) -> SliceOCRResult:
        """
        å¤„ç†å•ä¸ªåˆ‡ç‰‡çš„OCRè¯†åˆ«ï¼Œè®°å½•è¯¦ç»†çš„detectionå’Œrecognitionæ—¶é—´
        
        Args:
            slice_info: åˆ‡ç‰‡ä¿¡æ¯
            save_visualization: æ˜¯å¦ä¿å­˜å¯è§†åŒ–ç»“æœ
            
        Returns:
            åˆ‡ç‰‡OCRç»“æœï¼ŒåŒ…å«è¯¦ç»†è®¡æ—¶ä¿¡æ¯
        """
        slice_idx = slice_info.slice_index
        logger.info(f"å¼€å§‹è¯¦ç»†è®¡æ—¶å¤„ç†åˆ‡ç‰‡ {slice_idx}...")
        
        # åˆå§‹åŒ–è®¡æ—¶è®°å½•
        timing_record = {
            "slice_index": slice_idx,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "image_shape": slice_info.image.shape,
            "start_y": slice_info.start_y,
            "end_y": slice_info.end_y,
            # OCRé¢„å¤„ç†é˜¶æ®µ
            "ocr_preprocessing_time": 0.0,
            # Detectioné˜¶æ®µç»†åˆ†
            "detection_time": 0.0,
            "detection_detailed": {},
            # Recognitionå‰çš„è£å‰ªé¢„å¤„ç†
            "crop_preprocessing_time": 0.0,
            # Recognitioné˜¶æ®µç»†åˆ†
            "recognition_time": 0.0,
            "recognition_detailed": {},
            # æœ€ç»ˆåå¤„ç†
            "final_postprocessing_time": 0.0,
            # å¯é€‰é˜¶æ®µ
            "classification_time": 0.0,
            # å…¼å®¹æ€§å­—æ®µï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            "detection_postprocessing_time": 0.0,
            "recognition_preprocessing_time": 0.0,
            "recognition_postprocessing_time": 0.0,
            "preprocessing_time": 0.0,
            "postprocessing_time": 0.0,
            "total_ocr_time": 0.0,
            "detected_boxes_count": 0,
            "recognized_texts_count": 0,
            "final_texts_count": 0
        }
        
        # è®°å½•æ€»ä½“å¼€å§‹æ—¶é—´
        total_start_time = time.time()
        
        # åˆ›å»ºç»“æœå¯¹è±¡
        result = SliceOCRResult(
            slice_index=slice_idx,
            start_y=slice_info.start_y,
            end_y=slice_info.end_y
        )
        
        try:
            # 1. OCRå›¾åƒé¢„å¤„ç†ï¼ˆDetectionå’ŒRecognitionå…±ç”¨ï¼‰
            ocr_preprocess_start = time.time()
            slice_img_rgb = cv2.cvtColor(slice_info.image, cv2.COLOR_BGR2RGB)
            
            # å›¾åƒå°ºå¯¸è°ƒæ•´ï¼ˆæ¨¡æ‹ŸRapidOCRçš„é¢„å¤„ç†ï¼‰
            img, ratio_h, ratio_w = resize_image_within_bounds(
                slice_img_rgb, self.min_side_len, self.max_side_len
            )
            op_record = {"preprocess": {"ratio_h": ratio_h, "ratio_w": ratio_w}}
            
            timing_record["ocr_preprocessing_time"] = time.time() - ocr_preprocess_start
            
            # 2. æ–‡æœ¬æ£€æµ‹ (Detection) - ä½¿ç”¨è¯¦ç»†æ€§èƒ½åˆ†æ
            detection_start = time.time()
            if hasattr(self.text_det, 'get_last_detailed_timing'):
                det_res = self.text_det(img)
                detailed_detection_timing = self.text_det.get_last_detailed_timing()
                
                # è®°å½•è¯¦ç»†Detectionæ—¶é—´
                timing_record["detection_detailed"] = detailed_detection_timing
                timing_record["detection_time"] = detailed_detection_timing.get('total_time', time.time() - detection_start)
            else:
                det_res = self.text_det(img)
                timing_record["detection_time"] = time.time() - detection_start
                timing_record["detection_detailed"] = {}
            timing_record["detected_boxes_count"] = len(det_res.boxes) if det_res.boxes is not None else 0
            
            logger.info(f"åˆ‡ç‰‡ {slice_idx} - Detectionå®Œæˆ: {timing_record['detection_time']:.3f}s, æ£€æµ‹åˆ° {timing_record['detected_boxes_count']} ä¸ªæ–‡æœ¬æ¡†")
            
            # å¯è§†åŒ–detectionç»“æœ
            if det_res.boxes is not None and len(det_res.boxes) > 0:
                self._visualize_detection_results(img, det_res, slice_idx)
            
            if det_res.boxes is None or len(det_res.boxes) == 0:
                logger.warning(f"åˆ‡ç‰‡ {slice_idx} æœªæ£€æµ‹åˆ°æ–‡æœ¬æ¡†")
                timing_record["total_ocr_time"] = time.time() - total_start_time
                self.slice_timing_records[slice_idx] = timing_record
                return result
            
            # 3. Recognitionå‰é¢„å¤„ç†ï¼šåˆ‡å‰²æ–‡æœ¬åŒºåŸŸå›¾åƒ
            crop_preprocess_start = time.time()
            img_crop_list = []
            for box in det_res.boxes:
                tmp_box = copy.deepcopy(box)
                img_crop = get_rotate_crop_image(img, tmp_box)
                img_crop_list.append(img_crop)
            timing_record["crop_preprocessing_time"] = time.time() - crop_preprocess_start
            
            # 4. æ–‡æœ¬æ–¹å‘åˆ†ç±» (Classification) - å¯é€‰
            if hasattr(self.cfg.Global, 'use_cls') and self.cfg.Global.use_cls:
                cls_start = time.time()
                cls_res = self.text_cls(img_crop_list)
                timing_record["classification_time"] = time.time() - cls_start
                if cls_res.img_list is not None:
                    img_crop_list = cls_res.img_list
                logger.info(f"åˆ‡ç‰‡ {slice_idx} - Classificationå®Œæˆ: {timing_record['classification_time']:.3f}s")
            
            # 5. æ–‡æœ¬è¯†åˆ« (Recognition) - ä½¿ç”¨è¯¦ç»†æ€§èƒ½åˆ†æ
            recognition_start = time.time()
            rec_input = TextRecInput(img=img_crop_list, return_word_box=False)
            
            # ä½¿ç”¨åŒ…è£…å™¨è·å–è¯¦ç»†æ—¶é—´ä¿¡æ¯
            if hasattr(self.text_rec, 'get_last_detailed_timing'):
                rec_res = self.text_rec(rec_input)
                detailed_recognition_timing = self.text_rec.get_last_detailed_timing()
                
                # è®°å½•è¯¦ç»†Recognitionæ—¶é—´
                timing_record["recognition_detailed"] = detailed_recognition_timing
                timing_record["recognition_time"] = detailed_recognition_timing.get('total_time', time.time() - recognition_start)
            else:
                rec_res = self.text_rec(rec_input)
                timing_record["recognition_time"] = time.time() - recognition_start
                timing_record["recognition_detailed"] = {}
            
            timing_record["recognized_texts_count"] = len(rec_res.txts) if rec_res.txts is not None else 0
            
            logger.info(f"åˆ‡ç‰‡ {slice_idx} - Recognitionå®Œæˆ: {timing_record['recognition_time']:.3f}s, è¯†åˆ«åˆ° {timing_record['recognized_texts_count']} ä¸ªæ–‡æœ¬")
            
            # è®°å½•è¯¦ç»†æ€§èƒ½ä¿¡æ¯åˆ°æ—¥å¿—
            if timing_record.get("detection_detailed"):
                det_detailed = timing_record["detection_detailed"]
                logger.debug(f"  Detectionè¯¦ç»† - é¢„å¤„ç†: {det_detailed.get('preprocessing_time', 0):.3f}s, "
                           f"æ¨ç†: {det_detailed.get('inference_time', 0):.3f}s, "
                           f"åå¤„ç†: {det_detailed.get('postprocessing_time', 0):.3f}s")
            
            if timing_record.get("recognition_detailed"):
                rec_detailed = timing_record["recognition_detailed"]
                logger.debug(f"  Recognitionè¯¦ç»† - å‰å¤„ç†: {rec_detailed.get('preprocessing', {}).get('total', 0):.3f}s, "
                           f"æ¨ç†: {rec_detailed.get('forward', {}).get('inference_time', 0):.3f}s, "
                           f"åå¤„ç†: {rec_detailed.get('postprocessing', {}).get('total', 0):.3f}s")
            
            # 6. æœ€ç»ˆåå¤„ç†ï¼ˆåæ ‡è½¬æ¢ã€è¿‡æ»¤ç­‰ï¼‰
            final_postprocess_start = time.time()
            
            if det_res.boxes is not None and rec_res.txts is not None and rec_res.scores is not None:
                # è°ƒæ•´åæ ‡å›åŸå›¾åæ ‡ç³»
                dt_boxes_array = np.array(det_res.boxes).astype(np.float32)
                
                # åº”ç”¨é¢„å¤„ç†çš„é€†å˜æ¢
                for op in reversed(list(op_record.keys())):
                    v = op_record[op]
                    if "preprocess" in op:
                        ratio_h = v.get("ratio_h", 1.0)
                        ratio_w = v.get("ratio_w", 1.0)
                        dt_boxes_array[:, :, 0] *= ratio_w
                        dt_boxes_array[:, :, 1] *= ratio_h
                
                # ä¿å­˜ç”¨äºå¯è§†åŒ–çš„åˆ‡ç‰‡åæ ‡
                vis_boxes_slice = []
                vis_texts = []
                vis_scores = []
                
                # è¿‡æ»¤å’Œå¤„ç†OCRç»“æœ
                for box, txt, score in zip(dt_boxes_array, rec_res.txts, rec_res.scores):
                    # è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœ
                    if score < self.text_score_threshold:
                        continue
                    
                    # ä¿å­˜åˆ‡ç‰‡åæ ‡ç”¨äºå¯è§†åŒ–ï¼ˆè½¬æ¢ä¸ºæ•´æ•°ï¼‰
                    vis_boxes_slice.append([[int(x), int(y)] for x, y in box.tolist()])
                    vis_texts.append(txt)
                    vis_scores.append(score)
                    
                    # è½¬æ¢åæ ‡åˆ°åŸå›¾åæ ‡ç³»
                    adjusted_box = self._adjust_box_to_original(box.tolist(), slice_info.start_x, slice_info.start_y)
                    
                    # æ·»åŠ OCRé¡¹
                    result.add_ocr_item(
                        text=txt,
                        box=adjusted_box,
                        score=score
                    )
            
            # æ’åºç»“æœ
            result.sort_by_y()
            timing_record["final_texts_count"] = len(result.ocr_items)
            timing_record["final_postprocessing_time"] = time.time() - final_postprocess_start
            
            # å¯è§†åŒ–OCRç»“æœï¼ˆå¦‚æœæœ‰ç»“æœçš„è¯ï¼‰
            if vis_boxes_slice:
                try:
                    # ä½¿ç”¨åˆ‡ç‰‡åæ ‡è¿›è¡Œå¯è§†åŒ–
                    self._visualize_ocr_results(img, vis_boxes_slice, vis_texts, vis_scores, slice_idx)
                except Exception as vis_e:
                    logger.warning(f"åˆ‡ç‰‡ {slice_idx} OCRå¯è§†åŒ–å¤±è´¥: {vis_e}")
            
        except Exception as e:
            logger.error(f"åˆ‡ç‰‡ {slice_idx} å¤„ç†å‡ºé”™: {e}", exc_info=True)
        
        # è®¡ç®—æ€»æ—¶é—´
        timing_record["total_ocr_time"] = time.time() - total_start_time
        
        # è®¡ç®—å…¼å®¹æ€§å­—æ®µï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        timing_record["detection_postprocessing_time"] = timing_record["crop_preprocessing_time"]
        timing_record["recognition_preprocessing_time"] = timing_record["crop_preprocessing_time"]
        timing_record["recognition_postprocessing_time"] = timing_record["final_postprocessing_time"]
        timing_record["preprocessing_time"] = (timing_record["ocr_preprocessing_time"] + 
                                              timing_record["crop_preprocessing_time"])
        timing_record["postprocessing_time"] = timing_record["final_postprocessing_time"]
        
        # å­˜å‚¨è®¡æ—¶è®°å½•
        self.slice_timing_records[slice_idx] = timing_record
        
        # æ‰“å°è¯¦ç»†è®¡æ—¶ä¿¡æ¯
        self._print_slice_timing_detail(timing_record)
        
        logger.info(f"åˆ‡ç‰‡ {slice_idx} è¯¦ç»†è®¡æ—¶å¤„ç†å®Œæˆï¼Œæœ€ç»ˆè¯†åˆ«åˆ° {timing_record['final_texts_count']} ä¸ªæ–‡æœ¬")
        
        return result
    
    def _print_slice_timing_detail(self, timing_record: Dict):
        """æ‰“å°å•ä¸ªåˆ‡ç‰‡çš„è¯¦ç»†è®¡æ—¶ä¿¡æ¯"""
        idx = timing_record["slice_index"]
        total_time = timing_record['total_ocr_time']
        
        print(f"\nğŸ” åˆ‡ç‰‡ {idx} è¯¦ç»†è®¡æ—¶åˆ†æ:")
        print(f"  ğŸ“Š æ€»è€—æ—¶: {total_time:.3f}ç§’")
        
        # OCRé¢„å¤„ç†é˜¶æ®µ
        ocr_pre = timing_record['ocr_preprocessing_time']
        print(f"  ğŸ”§ OCRé¢„å¤„ç†: {ocr_pre:.3f}ç§’ ({ocr_pre/total_time*100:.1f}%)")
        
        # Detectioné˜¶æ®µ
        det_time = timing_record['detection_time']
        print(f"  ğŸ” Detectioné˜¶æ®µ: {det_time:.3f}ç§’ ({det_time/total_time*100:.1f}%)")
        if timing_record.get('detection_detailed'):
            det_detailed = timing_record['detection_detailed']
            det_prep = det_detailed.get('preprocessing_time', 0)
            det_inf = det_detailed.get('inference_time', 0)
            det_post = det_detailed.get('postprocessing_time', 0)
            print(f"    â”œâ”€ é¢„å¤„ç†: {det_prep:.3f}ç§’ ({det_prep/total_time*100:.1f}%)")
            print(f"    â”œâ”€ æ¨ç†: {det_inf:.3f}ç§’ ({det_inf/total_time*100:.1f}%)")
            print(f"    â””â”€ åå¤„ç†: {det_post:.3f}ç§’ ({det_post/total_time*100:.1f}%)")
        
        # è£å‰ªé¢„å¤„ç†
        crop_pre = timing_record['crop_preprocessing_time']
        print(f"  âœ‚ï¸  è£å‰ªé¢„å¤„ç†: {crop_pre:.3f}ç§’ ({crop_pre/total_time*100:.1f}%)")
        
        # å¯é€‰çš„åˆ†ç±»é˜¶æ®µ
        if timing_record['classification_time'] > 0:
            cls_time = timing_record['classification_time']
            print(f"  ğŸ”„ æ–¹å‘åˆ†ç±»: {cls_time:.3f}ç§’ ({cls_time/total_time*100:.1f}%)")
        
        # Recognitioné˜¶æ®µ
        rec_time = timing_record['recognition_time']
        print(f"  ğŸ“ Recognitioné˜¶æ®µ: {rec_time:.3f}ç§’ ({rec_time/total_time*100:.1f}%)")
        if timing_record.get('recognition_detailed'):
            rec_detailed = timing_record['recognition_detailed']
            rec_prep_total = rec_detailed.get('preprocessing', {}).get('total', 0)
            rec_inf = rec_detailed.get('forward', {}).get('inference_time', 0)
            rec_post_total = rec_detailed.get('postprocessing', {}).get('total', 0)
            print(f"    â”œâ”€ é¢„å¤„ç†: {rec_prep_total:.3f}ç§’ ({rec_prep_total/total_time*100:.1f}%)")
            print(f"    â”œâ”€ æ¨ç†: {rec_inf:.3f}ç§’ ({rec_inf/total_time*100:.1f}%)")
            print(f"    â””â”€ åå¤„ç†: {rec_post_total:.3f}ç§’ ({rec_post_total/total_time*100:.1f}%)")
        
        # æœ€ç»ˆåå¤„ç†
        final_post = timing_record['final_postprocessing_time']
        print(f"  ğŸ”„ æœ€ç»ˆåå¤„ç†: {final_post:.3f}ç§’ ({final_post/total_time*100:.1f}%)")
        
        print(f"  ğŸ“ˆ ç»“æœç»Ÿè®¡: æ£€æµ‹æ¡†{timing_record['detected_boxes_count']} â†’ è¯†åˆ«æ–‡æœ¬{timing_record['recognized_texts_count']} â†’ æœ€ç»ˆæ–‡æœ¬{timing_record['final_texts_count']}")
    
    def export_slice_timing_records(self):
        """å¯¼å‡ºæ‰€æœ‰åˆ‡ç‰‡çš„è¯¦ç»†è®¡æ—¶è®°å½•åˆ°JSONæ–‡ä»¶"""
        if not self.slice_timing_records:
            logger.warning("æ²¡æœ‰åˆ‡ç‰‡è®¡æ—¶è®°å½•å¯å¯¼å‡º")
            return
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        summary_stats = self._calculate_timing_summary()
        
        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "summary": summary_stats,
            "slice_details": self.slice_timing_records
        }
        
        # å¯¼å‡ºåˆ°æ–‡ä»¶
        output_file = self.timing_output_dir / "slice_ocr_detailed_timing.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"åˆ‡ç‰‡è¯¦ç»†è®¡æ—¶è®°å½•å·²å¯¼å‡ºåˆ°: {output_file}")
        print(f"ğŸ“ åˆ‡ç‰‡è¯¦ç»†è®¡æ—¶è®°å½•å·²ä¿å­˜åˆ°: {output_file}")
        
        return output_file
    
    def _calculate_timing_summary(self) -> Dict:
        """è®¡ç®—æ‰€æœ‰åˆ‡ç‰‡çš„è®¡æ—¶æ±‡æ€»ç»Ÿè®¡"""
        if not self.slice_timing_records:
            return {}
        
        records = list(self.slice_timing_records.values())
        total_slices = len(records)
        
        # æ±‡æ€»æ—¶é—´ - ç»†åˆ†ç»Ÿè®¡
        total_ocr_preprocessing = sum(r["ocr_preprocessing_time"] for r in records)
        total_detection = sum(r["detection_time"] for r in records)
        total_crop_preprocessing = sum(r["crop_preprocessing_time"] for r in records)
        total_recognition = sum(r["recognition_time"] for r in records)
        total_final_postprocessing = sum(r["final_postprocessing_time"] for r in records)
        total_classification = sum(r["classification_time"] for r in records)
        total_ocr = sum(r["total_ocr_time"] for r in records)
        
        # Detectionç»†åˆ†ç»Ÿè®¡
        total_det_preprocessing = sum(r.get("detection_detailed", {}).get("preprocessing_time", 0) for r in records)
        total_det_inference = sum(r.get("detection_detailed", {}).get("inference_time", 0) for r in records)
        total_det_postprocessing = sum(r.get("detection_detailed", {}).get("postprocessing_time", 0) for r in records)
        
        # Recognitionç»†åˆ†ç»Ÿè®¡
        total_rec_preprocessing = sum(r.get("recognition_detailed", {}).get("preprocessing", {}).get("total", 0) for r in records)
        total_rec_inference = sum(r.get("recognition_detailed", {}).get("forward", {}).get("inference_time", 0) for r in records)
        total_rec_postprocessing = sum(r.get("recognition_detailed", {}).get("postprocessing", {}).get("total", 0) for r in records)
        
        # å…¼å®¹æ€§å­—æ®µ
        total_preprocessing = sum(r["preprocessing_time"] for r in records)
        total_postprocessing = sum(r["postprocessing_time"] for r in records)
        
        # æ±‡æ€»æ•°é‡
        total_detected_boxes = sum(r["detected_boxes_count"] for r in records)
        total_recognized_texts = sum(r["recognized_texts_count"] for r in records)
        total_final_texts = sum(r["final_texts_count"] for r in records)
        
        # æ‰¾å‡ºæœ€è€—æ—¶å’Œæœ€å¿«çš„åˆ‡ç‰‡
        slowest_slice = max(records, key=lambda x: x["total_ocr_time"])
        fastest_slice = min(records, key=lambda x: x["total_ocr_time"])
        
        summary = {
            "total_slices": total_slices,
            "detailed_timing_summary": {
                # OCRé¢„å¤„ç†é˜¶æ®µ
                "total_ocr_preprocessing_time": round(total_ocr_preprocessing, 3),
                
                # Detectioné˜¶æ®µç»†åˆ†
                "total_detection_time": round(total_detection, 3),
                "total_detection_preprocessing_time": round(total_det_preprocessing, 3),
                "total_detection_inference_time": round(total_det_inference, 3),
                "total_detection_postprocessing_time": round(total_det_postprocessing, 3),
                
                # è£å‰ªé¢„å¤„ç†
                "total_crop_preprocessing_time": round(total_crop_preprocessing, 3),
                
                # Recognitioné˜¶æ®µç»†åˆ†
                "total_recognition_time": round(total_recognition, 3),
                "total_recognition_preprocessing_time": round(total_rec_preprocessing, 3),
                "total_recognition_inference_time": round(total_rec_inference, 3),
                "total_recognition_postprocessing_time": round(total_rec_postprocessing, 3),
                
                # æœ€ç»ˆåå¤„ç†
                "total_final_postprocessing_time": round(total_final_postprocessing, 3),
                
                # å¯é€‰é˜¶æ®µ
                "total_classification_time": round(total_classification, 3),
                
                # æ€»è®¡
                "total_ocr_time": round(total_ocr, 3),
                
                # å¹³å‡æ—¶é—´
                "average_ocr_preprocessing_time": round(total_ocr_preprocessing / total_slices, 3),
                "average_detection_time": round(total_detection / total_slices, 3),
                "average_detection_preprocessing_time": round(total_det_preprocessing / total_slices, 3),
                "average_detection_inference_time": round(total_det_inference / total_slices, 3),
                "average_detection_postprocessing_time": round(total_det_postprocessing / total_slices, 3),
                "average_crop_preprocessing_time": round(total_crop_preprocessing / total_slices, 3),
                "average_recognition_time": round(total_recognition / total_slices, 3),
                "average_recognition_preprocessing_time": round(total_rec_preprocessing / total_slices, 3),
                "average_recognition_inference_time": round(total_rec_inference / total_slices, 3),
                "average_recognition_postprocessing_time": round(total_rec_postprocessing / total_slices, 3),
                "average_final_postprocessing_time": round(total_final_postprocessing / total_slices, 3),
                "average_classification_time": round(total_classification / total_slices, 3),
                "average_total_time": round(total_ocr / total_slices, 3)
            },
            "legacy_timing_summary": {
                # å…¼å®¹æ€§å­—æ®µ
                "total_preprocessing_time": round(total_preprocessing, 3),
                "total_detection_time": round(total_detection, 3),
                "total_classification_time": round(total_classification, 3),
                "total_recognition_time": round(total_recognition, 3),
                "total_postprocessing_time": round(total_postprocessing, 3),
                "total_ocr_time": round(total_ocr, 3),
                
                "average_preprocessing_time": round(total_preprocessing / total_slices, 3),
                "average_detection_time": round(total_detection / total_slices, 3),
                "average_classification_time": round(total_classification / total_slices, 3),
                "average_recognition_time": round(total_recognition / total_slices, 3),
                "average_postprocessing_time": round(total_postprocessing / total_slices, 3),
                "average_total_time": round(total_ocr / total_slices, 3)
            },
            "processing_summary": {
                "total_detected_boxes": total_detected_boxes,
                "total_recognized_texts": total_recognized_texts,
                "total_final_texts": total_final_texts,
                "average_boxes_per_slice": round(total_detected_boxes / total_slices, 2),
                "average_texts_per_slice": round(total_final_texts / total_slices, 2),
                "detection_efficiency": round(total_detected_boxes / total_detection if total_detection > 0 else 0, 2),
                "recognition_efficiency": round(total_final_texts / total_recognition if total_recognition > 0 else 0, 2)
            },
            "performance_analysis": {
                "slowest_slice": {
                    "slice_index": slowest_slice["slice_index"],
                    "total_time": round(slowest_slice["total_ocr_time"], 3),
                    "detection_time": round(slowest_slice["detection_time"], 3),
                    "recognition_time": round(slowest_slice["recognition_time"], 3)
                },
                "fastest_slice": {
                    "slice_index": fastest_slice["slice_index"],
                    "total_time": round(fastest_slice["total_ocr_time"], 3),
                    "detection_time": round(fastest_slice["detection_time"], 3),
                    "recognition_time": round(fastest_slice["recognition_time"], 3)
                },
                "detailed_time_distribution": {
                    "ocr_preprocessing_percentage": round((total_ocr_preprocessing / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "detection_percentage": round((total_detection / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "detection_preprocessing_percentage": round((total_det_preprocessing / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "detection_inference_percentage": round((total_det_inference / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "detection_postprocessing_percentage": round((total_det_postprocessing / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "crop_preprocessing_percentage": round((total_crop_preprocessing / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "recognition_percentage": round((total_recognition / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "recognition_preprocessing_percentage": round((total_rec_preprocessing / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "recognition_inference_percentage": round((total_rec_inference / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "recognition_postprocessing_percentage": round((total_rec_postprocessing / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "final_postprocessing_percentage": round((total_final_postprocessing / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "classification_percentage": round((total_classification / total_ocr) * 100, 1) if total_ocr > 0 else 0
                },
                "legacy_time_distribution": {
                    "detection_percentage": round((total_detection / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "recognition_percentage": round((total_recognition / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "preprocessing_percentage": round((total_preprocessing / total_ocr) * 100, 1) if total_ocr > 0 else 0,
                    "postprocessing_percentage": round((total_postprocessing / total_ocr) * 100, 1) if total_ocr > 0 else 0
                }
            }
        }
        
        return summary
    
    def _visualize_ocr_results(self, img: np.ndarray, boxes: List, texts: List, 
                              scores: List, slice_idx: int, 
                              save_path: Optional[str] = None) -> np.ndarray:
        """
        å¯è§†åŒ–OCRç»“æœï¼Œç»˜åˆ¶æ£€æµ‹æ¡†ï¼ˆç»¿è‰²ï¼‰å’Œè¯†åˆ«æ–‡å­—ï¼ˆé»‘è‰²ï¼Œåœ¨æ¡†ä¸Šæ–¹ï¼‰
        
        Args:
            img: è¾“å…¥å›¾åƒ (RGBæ ¼å¼)
            boxes: æ£€æµ‹æ¡†åæ ‡åˆ—è¡¨
            texts: è¯†åˆ«æ–‡å­—åˆ—è¡¨  
            scores: ç½®ä¿¡åº¦åˆ†æ•°åˆ—è¡¨
            slice_idx: åˆ‡ç‰‡ç´¢å¼•
            save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            ç»˜åˆ¶äº†OCRç»“æœçš„å›¾åƒ
        """
        if not boxes or not texts:
            logger.warning(f"åˆ‡ç‰‡ {slice_idx} æ²¡æœ‰OCRç»“æœå¯å¯è§†åŒ–")
            return img
        
        # å¤åˆ¶å›¾åƒç”¨äºç»˜åˆ¶
        vis_img = img.copy()
        
        # è½¬æ¢ä¸ºPIL Imageä»¥ä¾¿æ›´å¥½åœ°å¤„ç†ä¸­æ–‡å­—ä½“
        pil_img = Image.fromarray(vis_img)
        draw = ImageDraw.Draw(pil_img)
        
        # å°è¯•åŠ è½½ä¸­æ–‡å­—ä½“
        try:
            # å°è¯•å‡ ä¸ªå¸¸è§çš„ä¸­æ–‡å­—ä½“è·¯å¾„
            font_paths = [
                "/home/kylin/æ¡Œé¢/Long-picture-ocr-LLMs-main_a/ShanHaiJiGuSongKe-JianFan-2.ttf",
                "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
                "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf", 
                "/System/Library/Fonts/PingFang.ttc",  # macOS
                "C:/Windows/Fonts/simhei.ttf",  # Windows
                "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc"  # Ubuntu
            ]
            font = None
            for font_path in font_paths:
                try:
                    if Path(font_path).exists():
                        font = ImageFont.truetype(font_path, size=20)
                        break
                except:
                    continue
            
            if font is None:
                font = ImageFont.load_default()
                logger.warning("ä½¿ç”¨é»˜è®¤å­—ä½“ï¼Œå¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡")
        except Exception as e:
            font = ImageFont.load_default()
            logger.warning(f"å­—ä½“åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
        
        # ç»˜åˆ¶æ¯ä¸ªæ£€æµ‹æ¡†å’Œå¯¹åº”çš„è¯†åˆ«æ–‡å­—
        for i, (box, text, score) in enumerate(zip(boxes, texts, scores)):
            # å°†boxè½¬æ¢ä¸ºæ•´æ•°åæ ‡
            if isinstance(box, np.ndarray):
                box = box.tolist()
            box = [[int(x), int(y)] for x, y in box]
            
            # ç»˜åˆ¶ç»¿è‰²æ£€æµ‹æ¡† - ä½¿ç”¨polygonç»˜åˆ¶å°é—­çš„å››è¾¹å½¢
            box_coords = [(x, y) for x, y in box]
            draw.polygon(box_coords, outline=(0, 255, 0), width=2)
            
            # è®¡ç®—æ–‡å­—ä½ç½®ï¼ˆæ¡†çš„ä¸Šæ–¹ï¼‰
            box_top_y = min([y for x, y in box])
            box_left_x = min([x for x, y in box]) 
            box_right_x = max([x for x, y in box])
            box_center_x = (box_left_x + box_right_x) // 2
            
            # è·å–æ–‡å­—å°ºå¯¸
            bbox = draw.textbbox((0, 0), text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            
            # æ–‡å­—ä½ç½®ï¼šåœ¨æ¡†ä¸Šæ–¹ï¼Œå±…ä¸­å¯¹é½ï¼Œç•™æœ‰è¾¹è·
            text_x = max(0, box_center_x - text_width // 2)
            text_y = max(0, box_top_y - text_height - 5)
            
            # ç¡®ä¿æ–‡å­—ä¸ä¼šè¶…å‡ºå›¾åƒè¾¹ç•Œ
            text_x = min(text_x, pil_img.width - text_width)
            
            # ç»˜åˆ¶æ–‡å­—èƒŒæ™¯ï¼ˆç™½è‰²åŠé€æ˜ï¼‰
            bg_margin = 2
            bg_coords = [
                text_x - bg_margin, text_y - bg_margin,
                text_x + text_width + bg_margin, text_y + text_height + bg_margin
            ]
            draw.rectangle(bg_coords, fill=(255, 255, 255, 200))
            
            # ç»˜åˆ¶é»‘è‰²æ–‡å­—
            draw.text((text_x, text_y), text, fill=(0, 0, 0), font=font)
            
            # åœ¨æ¡†çš„å·¦ä¸‹è§’ç»˜åˆ¶åºå·å’Œç½®ä¿¡åº¦
            info_text = f"{i}:{score:.3f}"
            info_bbox = draw.textbbox((0, 0), info_text, font=font)
            info_width = info_bbox[2] - info_bbox[0]
            info_height = info_bbox[3] - info_bbox[1]
            
            box_bottom_y = max([y for x, y in box])
            info_x = box_left_x
            info_y = min(pil_img.height - info_height, box_bottom_y + 2)
            
            # ç»˜åˆ¶ä¿¡æ¯èƒŒæ™¯
            info_bg_coords = [
                info_x - 2, info_y - 2,
                info_x + info_width + 2, info_y + info_height + 2
            ]
            draw.rectangle(info_bg_coords, fill=(0, 255, 0, 200))
            
            # ç»˜åˆ¶ç™½è‰²ä¿¡æ¯æ–‡å­—
            draw.text((info_x, info_y), info_text, fill=(255, 255, 255), font=font)
        
        # è½¬æ¢å›numpyæ•°ç»„
        vis_img = np.array(pil_img)
        
        # ä¿å­˜å›¾åƒ
        if save_path is None:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            debug_dir = Path("output_images/debug")
            debug_dir.mkdir(parents=True, exist_ok=True)
            save_path = debug_dir / f"slice_{slice_idx}_ocr_results.jpg"
        
        # è½¬æ¢å›BGRæ ¼å¼ç”¨äºä¿å­˜
        vis_img_bgr = cv2.cvtColor(vis_img, cv2.COLOR_RGB2BGR)
        cv2.imwrite(str(save_path), vis_img_bgr)
        
        logger.info(f"åˆ‡ç‰‡ {slice_idx} çš„OCRå¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
        print(f"ğŸ¨ åˆ‡ç‰‡ {slice_idx} OCRå¯è§†åŒ–å›¾å·²ä¿å­˜: {save_path}")
        print(f"   ğŸ“Š æ£€æµ‹æ¡†æ•°é‡: {len(boxes)}")
        print(f"   ğŸ“ è¯†åˆ«æ–‡å­—æ•°é‡: {len(texts)}")
        if scores:
            print(f"   ğŸ“ˆ å¹³å‡ç½®ä¿¡åº¦: {np.mean(scores):.3f}")
            print(f"   ğŸ† æœ€é«˜ç½®ä¿¡åº¦: {np.max(scores):.3f}")
            print(f"   ğŸ“‰ æœ€ä½ç½®ä¿¡åº¦: {np.min(scores):.3f}")
        
        return vis_img
    
    def visualize_full_image_ocr_results(self, original_image: np.ndarray, 
                                       all_ocr_items: List, save_path: Optional[str] = None) -> np.ndarray:
        """
        åœ¨åŸå›¾ä¸Šå¯è§†åŒ–æ‰€æœ‰OCRç»“æœ
        
        Args:
            original_image: åŸå§‹å›¾åƒ (BGRæ ¼å¼)
            all_ocr_items: æ‰€æœ‰OCRé¡¹çš„åˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            ç»˜åˆ¶äº†æ‰€æœ‰OCRç»“æœçš„åŸå›¾
        """
        if not all_ocr_items:
            logger.warning("æ²¡æœ‰OCRç»“æœå¯åœ¨åŸå›¾ä¸Šå¯è§†åŒ–")
            return original_image
        
        # è½¬æ¢ä¸ºRGBæ ¼å¼
        img_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
        
        # å‡†å¤‡å¯è§†åŒ–æ•°æ®
        boxes = []
        texts = []
        scores = []
        
        for item in all_ocr_items:
            boxes.append(item.box)
            texts.append(item.text)
            scores.append(item.score)
        
        # è½¬æ¢ä¸ºPIL Imageä»¥ä¾¿æ›´å¥½åœ°å¤„ç†ä¸­æ–‡å­—ä½“
        pil_img = Image.fromarray(img_rgb)
        draw = ImageDraw.Draw(pil_img)
        
        # å°è¯•åŠ è½½ä¸­æ–‡å­—ä½“
        try:
            font_paths = [
                "/home/kylin/æ¡Œé¢/Long-picture-ocr-LLMs-main_a/ShanHaiJiGuSongKe-JianFan-2.ttf",
                "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
                "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf", 
                "/System/Library/Fonts/PingFang.ttc",  # macOS
                "C:/Windows/Fonts/simhei.ttf",  # Windows
                "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc"  # Ubuntu
            ]
            font = None
            for font_path in font_paths:
                try:
                    if Path(font_path).exists():
                        font = ImageFont.truetype(font_path, size=16)  # åŸå›¾ç”¨è¾ƒå°å­—ä½“
                        break
                except:
                    continue
            
            if font is None:
                font = ImageFont.load_default()
                logger.warning("ä½¿ç”¨é»˜è®¤å­—ä½“ï¼Œå¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡")
        except Exception as e:
            font = ImageFont.load_default()
            logger.warning(f"å­—ä½“åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
        
        logger.info(f"å¼€å§‹åœ¨åŸå›¾ä¸Šç»˜åˆ¶ {len(boxes)} ä¸ªOCRç»“æœ...")
        
        # ç»˜åˆ¶æ¯ä¸ªOCRç»“æœ
        for i, (box, text, score) in enumerate(zip(boxes, texts, scores)):
            try:
                # å°†boxè½¬æ¢ä¸ºæ•´æ•°åæ ‡
                if isinstance(box, np.ndarray):
                    box = box.tolist()
                box = [[int(x), int(y)] for x, y in box]
                
                # ç»˜åˆ¶ç»¿è‰²æ£€æµ‹æ¡† - ä½¿ç”¨polygonç»˜åˆ¶å°é—­çš„å››è¾¹å½¢
                box_coords = [(x, y) for x, y in box]
                draw.polygon(box_coords, outline=(0, 255, 0), width=2)
                
                # è®¡ç®—æ–‡å­—ä½ç½®ï¼ˆæ¡†çš„ä¸Šæ–¹ï¼‰
                box_top_y = min([y for x, y in box])
                box_left_x = min([x for x, y in box]) 
                box_right_x = max([x for x, y in box])
                box_center_x = (box_left_x + box_right_x) // 2
                
                # è·å–æ–‡å­—å°ºå¯¸
                bbox = draw.textbbox((0, 0), text, font=font)
                text_width = bbox[2] - bbox[0]
                text_height = bbox[3] - bbox[1]
                
                # æ–‡å­—ä½ç½®ï¼šåœ¨æ¡†ä¸Šæ–¹ï¼Œå±…ä¸­å¯¹é½ï¼Œç•™æœ‰è¾¹è·
                text_x = max(0, box_center_x - text_width // 2)
                text_y = max(0, box_top_y - text_height - 3)
                
                # ç¡®ä¿æ–‡å­—ä¸ä¼šè¶…å‡ºå›¾åƒè¾¹ç•Œ
                text_x = min(text_x, pil_img.width - text_width)
                
                # ç»˜åˆ¶æ–‡å­—èƒŒæ™¯ï¼ˆç™½è‰²åŠé€æ˜ï¼‰
                bg_margin = 1
                bg_coords = [
                    text_x - bg_margin, text_y - bg_margin,
                    text_x + text_width + bg_margin, text_y + text_height + bg_margin
                ]
                draw.rectangle(bg_coords, fill=(255, 255, 255, 180))
                
                # ç»˜åˆ¶é»‘è‰²æ–‡å­—
                draw.text((text_x, text_y), text, fill=(0, 0, 0), font=font)
                
            except Exception as e:
                logger.warning(f"ç»˜åˆ¶ç¬¬ {i} ä¸ªOCRç»“æœæ—¶å‡ºé”™: {e}")
                continue
        
        # è½¬æ¢å›numpyæ•°ç»„
        vis_img = np.array(pil_img)
        
        # ä¿å­˜å›¾åƒ
        if save_path is None:
            save_path = Path("output_images") / "full_image_ocr_results.jpg"
            save_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è½¬æ¢å›BGRæ ¼å¼ç”¨äºä¿å­˜
        vis_img_bgr = cv2.cvtColor(vis_img, cv2.COLOR_RGB2BGR)
        cv2.imwrite(str(save_path), vis_img_bgr)
        
        logger.info(f"åŸå›¾OCRå¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
        print(f"ğŸ¨ åŸå›¾OCRå¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        print(f"   ğŸ“Š æ€»OCRç»“æœæ•°é‡: {len(all_ocr_items)}")
        if scores:
            print(f"   ğŸ“ˆ å¹³å‡ç½®ä¿¡åº¦: {np.mean(scores):.3f}")
            print(f"   ğŸ† æœ€é«˜ç½®ä¿¡åº¦: {np.max(scores):.3f}")
            print(f"   ğŸ“‰ æœ€ä½ç½®ä¿¡åº¦: {np.min(scores):.3f}")
        
        return vis_img_bgr
    
    def _visualize_detection_results(self, img: np.ndarray, det_res: TextDetOutput, 
                                   slice_idx: int, save_path: Optional[str] = None) -> np.ndarray:
        """
        å¯è§†åŒ–detectionç»“æœï¼Œç»˜åˆ¶æ£€æµ‹æ¡†å’Œç½®ä¿¡åº¦åˆ†æ•°
        
        Args:
            img: è¾“å…¥å›¾åƒ (RGBæ ¼å¼)
            det_res: detectionç»“æœ
            slice_idx: åˆ‡ç‰‡ç´¢å¼•
            save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            ç»˜åˆ¶äº†æ£€æµ‹æ¡†çš„å›¾åƒ
        """
        if det_res.boxes is None or det_res.scores is None:
            logger.warning(f"åˆ‡ç‰‡ {slice_idx} æ²¡æœ‰æ£€æµ‹ç»“æœå¯å¯è§†åŒ–")
            return img
        
        # å¤åˆ¶å›¾åƒç”¨äºç»˜åˆ¶
        vis_img = img.copy()
        
        # ç»˜åˆ¶æ¯ä¸ªæ£€æµ‹æ¡†å’Œåˆ†æ•°
        for i, (box, score) in enumerate(zip(det_res.boxes, det_res.scores)):
            # å°†boxè½¬æ¢ä¸ºæ•´æ•°åæ ‡
            box = np.array(box, dtype=np.int32)
            
            # é€‰æ‹©é¢œè‰² (æ ¹æ®ç½®ä¿¡åº¦åˆ†æ•°)
            if score >= 0.9:
                color = (0, 255, 0)  # é«˜ç½®ä¿¡åº¦ - ç»¿è‰²
            elif score >= 0.7:
                color = (0, 255, 255)  # ä¸­ç­‰ç½®ä¿¡åº¦ - é»„è‰²
            else:
                color = (0, 0, 255)  # ä½ç½®ä¿¡åº¦ - çº¢è‰²
            
            # ç»˜åˆ¶æ£€æµ‹æ¡†
            cv2.polylines(vis_img, [box], True, color, 2)
            
            # åœ¨æ¡†çš„å·¦ä¸Šè§’é™„è¿‘ç»˜åˆ¶åºå·å’Œç½®ä¿¡åº¦
            text_pos = (int(box[0][0]), int(box[0][1]) - 5)
            text = f"{i}:{score:.3f}"
            
            # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
            cv2.rectangle(vis_img, 
                         (text_pos[0], text_pos[1] - text_size[1] - 2),
                         (text_pos[0] + text_size[0], text_pos[1] + 2),
                         color, -1)
            
            # ç»˜åˆ¶ç™½è‰²æ–‡å­—
            cv2.putText(vis_img, text, text_pos, 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # ä¿å­˜å›¾åƒ
        if save_path is None:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            debug_dir = Path("output_images/debug")
            debug_dir.mkdir(parents=True, exist_ok=True)
            save_path = debug_dir / f"slice_{slice_idx}_detection_boxes.jpg"
        
        # è½¬æ¢å›BGRæ ¼å¼ç”¨äºä¿å­˜
        vis_img_bgr = cv2.cvtColor(vis_img, cv2.COLOR_RGB2BGR)
        cv2.imwrite(str(save_path), vis_img_bgr)
        
        logger.info(f"åˆ‡ç‰‡ {slice_idx} çš„detectionå¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
        print(f"ğŸ¨ Detectionå¯è§†åŒ–å›¾å·²ä¿å­˜: {save_path}")
        print(f"   ğŸ“Š æ£€æµ‹æ¡†æ•°é‡: {len(det_res.boxes)}")
        print(f"   ğŸ“ˆ å¹³å‡ç½®ä¿¡åº¦: {np.mean(det_res.scores):.3f}")
        print(f"   ğŸ† æœ€é«˜ç½®ä¿¡åº¦: {np.max(det_res.scores):.3f}")
        print(f"   ğŸ“‰ æœ€ä½ç½®ä¿¡åº¦: {np.min(det_res.scores):.3f}")
        
        return vis_img
    
    def _process_ocr_items(self, ocr_result: Any, slice_info: SliceInfo, 
                          result: SliceOCRResult):
        """
        å¤„ç†OCRè¯†åˆ«é¡¹
        
        Args:
            ocr_result: RapidOCRçš„è¯†åˆ«ç»“æœ
            slice_info: åˆ‡ç‰‡ä¿¡æ¯
            result: åˆ‡ç‰‡OCRç»“æœå¯¹è±¡
        """
        for box, txt, score in zip(ocr_result.boxes, ocr_result.txts, ocr_result.scores):
            # è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœ
            if score < self.text_score_threshold:
                continue
            
            # è½¬æ¢åæ ‡åˆ°åŸå›¾åæ ‡ç³»
            adjusted_box = self._adjust_box_to_original(box, slice_info.start_x, slice_info.start_y)
            
            # æ·»åŠ OCRé¡¹
            result.add_ocr_item(
                text=txt,
                box=adjusted_box,
                score=score
            )
        
        logger.debug(f"åˆ‡ç‰‡ {slice_info.slice_index} è¿‡æ»¤åä¿ç•™ "
                    f"{len(result.ocr_items)} ä¸ªæ–‡æœ¬")
    
    def _adjust_box_to_original(self, box: List[List[float]], 
                               start_x: int = 0, start_y: int = 0) -> List[List[float]]:
        """
        å°†åˆ‡ç‰‡åæ ‡è½¬æ¢ä¸ºåŸå›¾åæ ‡
        
        Args:
            box: åˆ‡ç‰‡ä¸­çš„è¾¹ç•Œæ¡†åæ ‡
            start_x: åˆ‡ç‰‡åœ¨åŸå›¾ä¸­çš„èµ·å§‹Xåæ ‡
            start_y: åˆ‡ç‰‡åœ¨åŸå›¾ä¸­çš„èµ·å§‹Yåæ ‡
            
        Returns:
            åŸå›¾åæ ‡ç³»ä¸­çš„è¾¹ç•Œæ¡†
        """
        adjusted_box = []
        for point in box:
            adjusted_point = [point[0] + start_x, point[1] + start_y]
            adjusted_box.append(adjusted_point)
        return adjusted_box
    
    def batch_process_slices(self, slice_infos: List[SliceInfo]) -> List[SliceOCRResult]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªåˆ‡ç‰‡
        
        Args:
            slice_infos: åˆ‡ç‰‡ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            åˆ‡ç‰‡OCRç»“æœåˆ—è¡¨
        """
        results = []
        for slice_info in slice_infos:
            result = self.process_slice(slice_info)
            results.append(result)
        return results